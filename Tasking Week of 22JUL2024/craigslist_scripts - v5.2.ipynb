{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca09e35",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .main-header {\n",
    "        font-family: Courier, monospace;\n",
    "        font-size: 96px; /* Increased font size */\n",
    "        text-align: center;\n",
    "        color: #00FF00;\n",
    "    }\n",
    "    .sub-header {\n",
    "        font-family: Courier, monospace;\n",
    "        font-size: 72px; /* Increased font size */\n",
    "        text-align: center;\n",
    "        color: #00FF00;\n",
    "    }\n",
    "    .cursor {\n",
    "        font-family: Courier, monospace;\n",
    "        font-size: 72px; /* Increased font size to match sub-header */\n",
    "        text-align: center;\n",
    "        color: #00FF00;\n",
    "        display: inline;\n",
    "        animation: blink 1s steps(2, start) infinite;\n",
    "    }\n",
    "    @keyframes blink {\n",
    "        to {\n",
    "            visibility: hidden;\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"main-header\">\n",
    "    THOR - Site Script\n",
    "</div>\n",
    "<div class=\"sub-header\">\n",
    "    craigslist.com<span class=\"cursor\">_</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bd1fd-29d2-4d77-9348-0760aba034af",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e80c3-0401-45a9-b65e-b56332075b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "# import import_ipynb\n",
    "# import thor_filters\n",
    "\n",
    "# Setting pandas display options for better readability during debugging\n",
    "pd.set_option('display.max_columns', None)  # Display all columns in DataFrames\n",
    "pd.set_option('display.max_rows', None)     # Display all rows in DataFrames\n",
    "pd.options.mode.chained_assignment = None   # Disable warning for chained assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531ed6d-8de9-4c2c-9b11-e33c9749c6f7",
   "metadata": {},
   "source": [
    "## All Craigslist Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f95073-47a1-4ccf-ae14-dd72df5c9e19",
   "metadata": {},
   "source": [
    "### Check If Account Is blocked or Suspended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fd15a-df12-4f20-b243-b859f0bfd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proxy(session, ip, timeout=5):\n",
    "    \"\"\"\n",
    "    Check if the proxy is working by comparing the reported IP with the expected IP.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): The session with the proxy set.\n",
    "        ip (str): The expected IP address of the proxy.\n",
    "        timeout (int, optional): The timeout for the request in seconds. Default is 5 seconds.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the proxy IP matches the reported IP, False otherwise.\n",
    "    \"\"\"\n",
    "    print('Function: check_proxy')\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = session.get('https://icanhazip.com', timeout=timeout)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        reported_ip = response.text.strip()\n",
    "        print(f'Successful response from icanhazip.com. Reported IP: {reported_ip}')\n",
    "    except requests.Timeout:\n",
    "        print(f'Timeout error: The request took longer than {timeout} seconds.')\n",
    "        reported_ip = ''\n",
    "    except requests.RequestException as e:\n",
    "        print(f'Error setting proxy or fetching IP: {e}')\n",
    "        reported_ip = ''\n",
    "\n",
    "    print(f'Proxy IP: {ip} - Reported IP: {reported_ip}')\n",
    "    return ip.strip() == reported_ip\n",
    "\n",
    "\n",
    "def check_blocked(session, timeout=5):\n",
    "    print('Function: check_blocked')\n",
    "\n",
    "\n",
    "    checkpoint_data = {\n",
    "        \"ErrorName\": \"Blocked\",\n",
    "        \"ErrorTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"ErrorDescription\": \"Error loading search page\",\n",
    "    }\n",
    "\n",
    "    url = \"https://sapi.craigslist.org/web/v8/postings/search\"\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"cl_b=4|5411c775aa66ff242fc29367d82280f3412d3b62|17197899267o-9M; cl_tocmode=\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Host\": \"sapi.craigslist.org\",\n",
    "        \"If-Modified-Since\": \"Tue, 02 Jul 2024 14:59:49 GMT\",\n",
    "        \"Origin\": \"https://wichita.craigslist.org\",\n",
    "        \"Referer\": \"https://wichita.craigslist.org/\",\n",
    "        \"Sec-Fetch-Dest\": \"empty\",\n",
    "        \"Sec-Fetch-Mode\": \"cors\",\n",
    "        \"Sec-Fetch-Site\": \"same-site\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        \"sec-ch-ua\": '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": '\"Windows\"'\n",
    "    }\n",
    "    params = {\n",
    "        # \"subarea\": \"-\",\n",
    "        \"cc\": \"US\",\n",
    "        \"batchSize\": \"1\",\n",
    "        \"area_id\": \"99\",\n",
    "        \"lang\": \"en\",\n",
    "        # \"minDate\": \"$minDate\",\n",
    "        \"searchPath\": \"cta\",\n",
    "        \"startIndex\": \"0\",\n",
    "        # \"sort\": \"date\",\n",
    "        # \"bundleDuplicates\": \"1\",\n",
    "        # \"query\": \"diesel\"\n",
    "        # \"search_distance\": \"1000\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, params=params, timeout=timeout)\n",
    "        json_data = response.json()\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "    except requests.Timeout:\n",
    "        print(f'Timeout error: The request took longer than {timeout} seconds.')\n",
    "        checkpoint_data[\"ErrorDescription\"] = \"Timeout error while loading search page\"\n",
    "        return True, checkpoint_data\n",
    "    except requests.RequestException as e:\n",
    "        json_data = {}\n",
    "        print(f'Error fetching IP: {e}')\n",
    "        return True, checkpoint_data\n",
    "\n",
    "    try:\n",
    "        print(f\"Successful response from Craigslist Api: {len(json_data['data']['items'])}\")\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f'Error parsing response JSON: {e}')\n",
    "        return True, checkpoint_data\n",
    "\n",
    "    return False, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb81418-ca2d-4628-9640-75c58f689da7",
   "metadata": {},
   "source": [
    "### Craigslist Searching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45225892-89fd-4dda-81bb-a1090c1ab167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(dict_list, key='id'):\n",
    "    seen_ids = set()\n",
    "    unique_dicts = []\n",
    "    for dictionary in dict_list:\n",
    "        dict_id = dictionary.get(key)\n",
    "        if dict_id not in seen_ids:\n",
    "            unique_dicts.append(dictionary)\n",
    "            seen_ids.add(dict_id)\n",
    "    return unique_dicts\n",
    "\n",
    "\n",
    "def DecodeCraigslistData(json_data):\n",
    "    maxPostedDate = json_data['data']['decode']['maxPostedDate']\n",
    "    minDate = json_data['data']['decode']['minDate']\n",
    "    minPostedDate = json_data['data']['decode']['minPostedDate']\n",
    "    minPostingId = json_data['data']['decode']['minPostingId']\n",
    "    locationDescriptions = json_data['data']['decode']['locationDescriptions']\n",
    "    locations = json_data['data']['decode']['locations']\n",
    "    areas = json_data['data']['areas']\n",
    "    categories = {\n",
    "        '145': \"cto\",\n",
    "        '146': \"ctd\"\n",
    "    }\n",
    "    category_descriptions = {\n",
    "        '145': \"cars & trucks - by owner\",\n",
    "        '146': \"cars & trucks - by dealer\"\n",
    "    }\n",
    "    items = json_data['data']['items']\n",
    "\n",
    "\n",
    "    decoded_items = []\n",
    "    for item in items:\n",
    "        \n",
    "        #Get the Posting ID\n",
    "        PostingId = item[0] + minPostingId\n",
    "\n",
    "        #get posted date\n",
    "        PostedDate_from_min = item[1] + minPostedDate\n",
    "\n",
    "        #decode location string\n",
    "        location_string = item[4]\n",
    "        location_id_string, location_lat, location_long = location_string.split('~')\n",
    "\n",
    "        #parse and lookup location\n",
    "        location = locations[int(location_id_string.split(\":\")[0])]\n",
    "\n",
    "        #lookup Area name\n",
    "        area_name = areas[str(location[0])]['name']\n",
    "\n",
    "        #lookup location description\n",
    "        location_description = locationDescriptions[int(location_id_string.split(\":\")[1])]\n",
    "\n",
    "        #get catagory code\n",
    "        catagory_code = item[2]\n",
    "\n",
    "        #get price\n",
    "        price_int = item[3]\n",
    "\n",
    "        #get optional data\n",
    "        listing_url_part = ''\n",
    "        listing_miles = None\n",
    "        listing_price = None\n",
    "        listing_images = []\n",
    "        for item_list in item:\n",
    "            if isinstance(item_list, list) and len(item_list) > 0 and item_list[0] == 6:\n",
    "                listing_url_part = item_list[1]\n",
    "\n",
    "            if isinstance(item_list, list) and len(item_list) > 0 and item_list[0] == 10:\n",
    "                listing_price = item_list[1]\n",
    "\n",
    "            if isinstance(item_list, list) and len(item_list) > 0 and item_list[0] == 4:\n",
    "                listing_images = item_list[1:]\n",
    "                listing_images = ['https://images.craigslist.org/' + item.split(\":\", 1)[1] + '_1200x900.jpg' for item in listing_images]\n",
    "\n",
    "            if isinstance(item_list, list) and len(item_list) > 0 and item_list[0] == 9:\n",
    "                listing_miles = item_list[1]\n",
    "                \n",
    "\n",
    "        #Get title\n",
    "        if isinstance(item[-1], str):\n",
    "            listing_title = item[-1]\n",
    "        else:\n",
    "            listing_title = ''\n",
    "\n",
    "        #Build Link\n",
    "        if len(location) == 3:\n",
    "            # get location sub area\n",
    "            location_sub_area = location[2]\n",
    "\n",
    "            listing_link = 'https://' + location[1] + '.craigslist.org/'  + location_sub_area + '/' + categories[str(catagory_code)] + '/d/' + listing_url_part + '/' + str(PostingId) + '.html'\n",
    "        elif len(location) == 2:\n",
    "            location_sub_area = None\n",
    "            listing_link = 'https://' + location[1] + '.craigslist.org/' + categories[str(catagory_code)] + '/d/' + listing_url_part + '/' + str(PostingId) + '.html'\n",
    "        else:\n",
    "            listing_link = ''\n",
    "\n",
    "        #Add data to dataframe\n",
    "        decoded_data = {\n",
    "            'id': PostingId,\n",
    "            'PostingId': PostingId,\n",
    "            'Title': listing_title,\n",
    "            'PostingName': listing_url_part,\n",
    "            'PostedDate': PostedDate_from_min,\n",
    "            'AreaName':area_name,\n",
    "            'SubAreaName': location_sub_area,           \n",
    "            'locationDescription' :location_description,\n",
    "            'LocationLat': location_lat,\n",
    "            'LocationLong': location_long,\n",
    "            'CategoryId': catagory_code,\n",
    "            'CategoryCode': categories[str(catagory_code)],\n",
    "            'CategoryDescription': category_descriptions[str(catagory_code)],\n",
    "            'Miles': listing_miles,\n",
    "            'Price': price_int,\n",
    "            'PriceFormatted': listing_price,\n",
    "            'Images': listing_images,\n",
    "            'Link': listing_link\n",
    "        }\n",
    "\n",
    "        decoded_items.append(decoded_data)\n",
    "    return decoded_items\n",
    "\n",
    "\n",
    "\n",
    "def GetSearchListings(session, search_terms, timeout=(5, 5), sleep_time=3, max_pages=10):\n",
    "    print('Function: GetSearchListings')\n",
    "    errors = []\n",
    "\n",
    "    search_postal_codes = [\n",
    "        '67207',\n",
    "        '20009',\n",
    "        '93728'\n",
    "    ]\n",
    "\n",
    "    url = \"https://sapi.craigslist.org/web/v8/postings/search/full\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        \"Cookie\": \"cl_b=4|5411c775aa66ff242fc29367d82280f3412d3b62|17197899267o-9M; cl_tocmode=\",\n",
    "        \"accept\": \"*/*\",\n",
    "        \"accept-language\": \"en-US,en;q=0.9\",\n",
    "        \"cache-control\": \"no-cache\",\n",
    "        \"pragma\": \"no-cache\",\n",
    "        \"sec-ch-ua\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Google Chrome\\\";v=\\\"126\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-site\",\n",
    "        \"referer\": \"https://wichita.craigslist.org/\"\n",
    "    }\n",
    "\n",
    "    search_results_json = []\n",
    "    page_num = 1 \n",
    "    for postal_code in search_postal_codes:\n",
    "\n",
    "        params = {\n",
    "            \"batch\": \"99-0-360-1-0\",\n",
    "            \"bundleDuplicates\": \"1\",\n",
    "            \"cc\": \"US\",\n",
    "            \"lang\": \"en\",\n",
    "            \"postal\": postal_code,\n",
    "            \"query\": search_terms['Search Text'],\n",
    "            \"searchPath\": \"cta\",\n",
    "            \"search_distance\": \"1000\",\n",
    "            \"sort\": \"date\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, params=params, timeout=timeout)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            print(f'Successful response for page {page_num}')\n",
    "        except requests.Timeout:\n",
    "            error_info = {\n",
    "                \"ErrorName\": \"Timeout\",\n",
    "                \"ErrorTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"ErrorDescription\": f'Timeout error on page {page_num}'\n",
    "            }\n",
    "            print(error_info[\"ErrorDescription\"])\n",
    "            errors.append(error_info)\n",
    "            break  # Exit loop on timeout\n",
    "        except requests.RequestException as e:\n",
    "            error_info = {\n",
    "                \"ErrorName\": \"RequestException\",\n",
    "                \"ErrorTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"ErrorDescription\": f'Error on page {page_num}: {e}'\n",
    "            }\n",
    "            print(error_info[\"ErrorDescription\"])\n",
    "            errors.append(error_info)\n",
    "            break  # Exit loop on request exception\n",
    "\n",
    "        try:\n",
    "            json_data = response.json()\n",
    "            results_data = DecodeCraigslistData(json_data)\n",
    "            print(f'Page Returned Data: {len(results_data)}')\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            error_info = {\n",
    "                \"ErrorName\": \"JSONDecodeError\" if isinstance(e, json.JSONDecodeError) else \"KeyError\",\n",
    "                \"ErrorTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"ErrorDescription\": f'Error parsing response JSON on page {page_num}: {e}'\n",
    "            }\n",
    "            print(error_info[\"ErrorDescription\"])\n",
    "            errors.append(error_info)\n",
    "            break  # Exit loop on JSON error\n",
    "\n",
    "        #append list together\n",
    "        search_results_json = [*search_results_json, *results_data]\n",
    "\n",
    "        if not len(results_data):\n",
    "            print('No results returned')\n",
    "            break # Exit loop on no results\n",
    "        elif page_num < len(search_postal_codes):\n",
    "            page_num += 1\n",
    "            time.sleep(sleep_time) # Sleep only if there are more results\n",
    "            \n",
    "\n",
    "    #remove any duplicates\n",
    "    print('remove_duplicates', len(search_results_json))\n",
    "    search_results_json = remove_duplicates(search_results_json)\n",
    "    print('remaining values', len(search_results_json))\n",
    "    \n",
    "    return search_results_json, errors\n",
    "\n",
    "\n",
    "\n",
    "def GetListingDetails(session, listing_ids, timeout=(5, 5), sleep_time=3):\n",
    "\n",
    "    errors = []\n",
    "    # listing_details = pd.DataFrame()\n",
    "    listing_details_json = []\n",
    "    page_num = 1\n",
    "    for listing_info in listing_ids:\n",
    "\n",
    "        if not listing_info['SubAreaName']:\n",
    "            listing_info['SubAreaName'] = '-'\n",
    "            \n",
    "        url = f\"https://rapi.craigslist.org/web/v8/postings/{listing_info['AreaName']}/{listing_info['SubAreaName']}/{listing_info['CategoryCode']}/{listing_info['id']}\"\n",
    "        params = {\n",
    "            \"categoryAbbr\": listing_info['CategoryCode'],\n",
    "            \"cc\": \"US\",\n",
    "            \"hostname\": listing_info['AreaName'],\n",
    "            \"lang\": \"en\",\n",
    "            \"subareaAbbr\": listing_info['SubAreaName']\n",
    "        }\n",
    "\n",
    "\n",
    "        headers = {\n",
    "            \"Accept\": \"*/*\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cookie\": \"cl_b=4|5411c775aa66ff242fc29367d82280f3412d3b62|17197899267o-9M; cl_tocmode=\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Host\": \"sapi.craigslist.org\",\n",
    "            \"If-Modified-Since\": \"Tue, 02 Jul 2024 14:59:49 GMT\",\n",
    "            \"Origin\": \"https://wichita.craigslist.org\",\n",
    "            \"Referer\": \"https://wichita.craigslist.org/\",\n",
    "            \"Sec-Fetch-Dest\": \"empty\",\n",
    "            \"Sec-Fetch-Mode\": \"cors\",\n",
    "            \"Sec-Fetch-Site\": \"same-site\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "            \"sec-ch-ua\": '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Google Chrome\";v=\"126\"',\n",
    "            \"sec-ch-ua-mobile\": \"?0\",\n",
    "            \"sec-ch-ua-platform\": '\"Windows\"'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, params=params, timeout=timeout)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            print(f'Successful response for page')\n",
    "        except requests.Timeout:\n",
    "            error_info = {\n",
    "                \"ErrorName\": \"Timeout\",\n",
    "                \"ErrorTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"ErrorDescription\": f'Timeout error on page'\n",
    "            }\n",
    "            print(error_info[\"ErrorDescription\"])\n",
    "            errors.append(error_info)\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "            error_info = {\n",
    "                \"ErrorName\": \"RequestException\",\n",
    "                \"ErrorTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"ErrorDescription\": f'Error on page: {e}'\n",
    "            }\n",
    "            print(error_info[\"ErrorDescription\"])\n",
    "            errors.append(error_info)\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            json_data = response.json()\n",
    "            json_items = json_data['data']['items']\n",
    "            results_returned = len(json_items)\n",
    "            print(f'Returned: {results_returned} results. {page_num} out of {len(listing_ids)} total.')\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            error_info = {\n",
    "                \"ErrorName\": \"JSONDecodeError\" if isinstance(e, json.JSONDecodeError) else \"KeyError\",\n",
    "                \"ErrorTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"ErrorDescription\": f'Error parsing response JSON: {e}'\n",
    "            }\n",
    "            print(error_info[\"ErrorDescription\"])\n",
    "            errors.append(error_info)\n",
    "            break\n",
    "\n",
    "\n",
    "        #append list together\n",
    "        listing_details_json = [*listing_details_json, *json_items]\n",
    "\n",
    "        if page_num < len(listing_ids):\n",
    "            page_num += 1\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    #remove any duplicates\n",
    "    print('remove_duplicates', len(listing_details_json))\n",
    "    listing_details_json = remove_duplicates(listing_details_json, key='postingId')\n",
    "    print('remaining values', len(listing_details_json))\n",
    "\n",
    "    return listing_details_json, errors   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d77f6",
   "metadata": {},
   "source": [
    "## Standard Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FormatAggregator(ids, results, details, custom, source_key):\n",
    "    \"\"\"\n",
    "    Aggregates and formats data from multiple DataFrames based on provided IDs and source key mapping.\n",
    "\n",
    "    Parameters:\n",
    "    ids (list): List of IDs to iterate over and extract data for.\n",
    "    results (pd.DataFrame): DataFrame containing results data (not directly used in the function but assumed to be part of locals()).\n",
    "    details (pd.DataFrame): DataFrame containing details data (not directly used in the function but assumed to be part of locals()).\n",
    "    custom (pd.DataFrame): Default DataFrame to use if no specific DataFrame is mentioned in source_key.\n",
    "    source_key (dict): Dictionary mapping target DataFrame columns to source DataFrame columns. The key is the target column name, and the value is a dictionary with the source DataFrame name as key and source column name as value. If value is an empty dictionary, the target column will be filled with None.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the aggregated and formatted data.\n",
    "    \n",
    "    Example:\n",
    "    source_key = {\n",
    "        'column1': {'results': 'source_column1'},\n",
    "        'column2': {'details': 'source_column2'},\n",
    "        'column3': {},  # This will be filled with None\n",
    "        'column4': 'custom',  # This will default to the custom DataFrame\n",
    "    }\n",
    "\n",
    "    Notes:\n",
    "    - The function assumes that the DataFrames (results, details) are present in the local scope.\n",
    "    - If a source column specified in source_key is not present in the corresponding DataFrame, the resulting column will be filled with None.\n",
    "    - The function concatenates the formatted data into a single DataFrame.\n",
    "    \"\"\"\n",
    "    print('Function: FormatAggregator')\n",
    "\n",
    "\n",
    "    # Create an empty DataFrame with columns from the keys of source_key\n",
    "    formatted_df = pd.DataFrame(columns=source_key.keys())\n",
    "    formatted_data = []\n",
    "    \n",
    "    # Iterate over each id\n",
    "    for id_ in ids:\n",
    "        row = {}\n",
    "        # Iterate over each key in source_key\n",
    "        for key, value in source_key.items():\n",
    "            if not value:  # If value is an empty dictionary\n",
    "                row[key] = value\n",
    "                continue\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                # Extract the DataFrame and column name\n",
    "                df_name, column_name = list(value.items())[0]\n",
    "                df = locals().get(df_name)  # Get the DataFrame by its name\n",
    "                \n",
    "                if df is None or column_name not in df.columns:\n",
    "                    row[key] = None\n",
    "                else:\n",
    "                    # Extract the value from the DataFrame using the id\n",
    "                    match = df.loc[df['id'] == id_, column_name]\n",
    "                    row[key] = match.values[0] if not match.empty else None\n",
    "            else:\n",
    "                if key == 'id': # If id, use the current id\n",
    "                    row[key] = id_\n",
    "                else:\n",
    "                    # If the value is not a dictionary, use the provided value directly\n",
    "                    row[key] = value\n",
    "        \n",
    "        # Append the row to the formatted DataFrame\n",
    "        formatted_data.append(row)\n",
    "    \n",
    "    formatted_df = pd.concat([formatted_df, pd.DataFrame(formatted_data)])\n",
    "    \n",
    "    return formatted_df\n",
    "\n",
    "\n",
    "\n",
    "def Format_Output(current_task, current_user, search_results, listing_details, ids):\n",
    "    \"\"\"\n",
    "    Formats and aggregates output data for vehicle listings based on various input parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - current_task (dict): Information about the current task, including task name and host.\n",
    "    - current_user (dict): Information about the current user, including account details.\n",
    "    - search_results (DataFrame): DataFrame containing search results with details such as URL, creation time, fuel type, etc.\n",
    "    - listing_details (DataFrame): DataFrame containing detailed information about listings, including photos, descriptions, and location.\n",
    "    - ids (list): List of IDs corresponding to the vehicle listings.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following elements:\n",
    "        - search_results (DataFrame): The original search results DataFrame.\n",
    "        - listing_details (DataFrame): The original listing details DataFrame.\n",
    "        - Listings (DataFrame): Aggregated and formatted listings DataFrame.\n",
    "        - task_telemetry (DataFrame): DataFrame containing task telemetry information.\n",
    "        - user_telemetry (DataFrame): DataFrame containing user telemetry information.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses a source key dictionary to map specific fields to their corresponding sources (custom, results, or details).\n",
    "    - It ensures required columns are present in the listing details before processing.\n",
    "    - Custom data is created by extracting and transforming specific fields such as images, title, and location.\n",
    "    - The final Listings DataFrame is generated using the FormatAggregator function.\n",
    "    - Task telemetry and user telemetry data are also created and returned as part of the output.\n",
    "    \"\"\"\n",
    "    print('Function: Format_Output')\n",
    "\n",
    "    \n",
    "    source_key = {\n",
    "        'title': {'details': 'title'},\n",
    "        'id': 'id',\n",
    "        'link': {'details': 'url'},\n",
    "        'creation_time': {'details': 'postedDate'},\n",
    "        'vehicle_condition': {'details': 'condition'},\n",
    "        'vehicle_color': {'details': 'auto_paint'},\n",
    "        'fuel_type': {'details': 'auto_fuel_type'},\n",
    "        'paid_off': {'details': 'auto_title_status'},\n",
    "        'make': {'details': 'auto_make_model'},\n",
    "        'model': {'details': 'auto_make_model'},\n",
    "        'year': {'details': 'auto_year'},\n",
    "        'number_owners': '',\n",
    "        'seller_type': {'details': 'categoryAbbr'},\n",
    "        'vehicle_trim': '',\n",
    "        'vin': {'details': 'auto_vin'},\n",
    "        'listing_photos': {'details': 'images'},\n",
    "        'seller_name': {'results': 'locationDescription'},\n",
    "        'location': {'details': 'location.description'},\n",
    "        'location_city': {'details': 'location.area'},\n",
    "        'location_state': '',\n",
    "        'location_country': '',\n",
    "        'description': {'details': 'body'},\n",
    "        'price': {'details': 'price'},\n",
    "        'strikethrough_price': '',\n",
    "        'odometer_unit': '',\n",
    "        'odometer_value': {'details': 'auto_miles'},\n",
    "        'thor_timestamp': {'results': 'thor_timestamp'},\n",
    "        'thor_website': \"autotrader.com\",\n",
    "        'thor_mmr': False,\n",
    "        'thor_task': f\"{current_task}\",\n",
    "        'thor_user': f\"{current_user}\",\n",
    "        'Task': current_task['TaskName'],\n",
    "        'Host': current_task[\"Host\"],\n",
    "        'Account': current_user[\"AccountInfo\"][\"AccountID\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    if not search_results.empty:\n",
    "        #Custome Data from search_results here\n",
    "        custom_data = pd.DataFrame(columns=['id'])\n",
    "    else:\n",
    "        custom_data = pd.DataFrame(columns=['id'])\n",
    "\n",
    "\n",
    "    if not listing_details.empty:\n",
    "        custom_data = pd.merge(listing_details, custom_data, on='id', how='left')\n",
    "\n",
    "        # Check if 'attributes' column exists and expand the attributes\n",
    "        if 'attributes' in listing_details.columns:\n",
    "            try:\n",
    "                listing_details = listing_details.join(\n",
    "                    listing_details['attributes'].apply(\n",
    "                        lambda row: pd.Series(\n",
    "                            {\n",
    "                                attr['postingAttributeKey']: attr['value']\n",
    "                                for attr in row\n",
    "                            }\n",
    "                        ) if isinstance(row, list) else pd.Series()\n",
    "                    )\n",
    "                ).drop(columns=['attributes'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing 'attributes' column: {e}\")\n",
    "\n",
    "        # Check if 'autoVinData' column exists and expand autoVinData\n",
    "        if 'autoVinData' in listing_details.columns:\n",
    "            try:\n",
    "                listing_details = listing_details.join(\n",
    "                    listing_details['autoVinData'].apply(\n",
    "                        lambda row: pd.Series(\n",
    "                            {\n",
    "                                f\"{main_category[0]}.{attribute[0]}\": attribute[1]\n",
    "                                for main_category in row\n",
    "                                for attribute in main_category[1]\n",
    "                            }\n",
    "                        ) if isinstance(row, list) else pd.Series()\n",
    "                    )\n",
    "                ).drop(columns=['autoVinData'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing 'autoVinData' column: {e}\")\n",
    "\n",
    "        # Check if 'images' column exists and expand images\n",
    "        if 'images' in listing_details.columns:\n",
    "            try:\n",
    "                listing_details['images'] = listing_details['images'].apply(\n",
    "                    lambda listing_images: [\n",
    "                        f\"https://images.craigslist.org/{item.split(':', 1)[1]}_1200x900.jpg\" \n",
    "                        for item in listing_images\n",
    "                    ] if isinstance(listing_images, list) else []\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing 'images' column: {e}\")\n",
    "    else:\n",
    "        print('no details')\n",
    "\n",
    "\n",
    "    Listings = FormatAggregator(ids, search_results, listing_details, custom_data, source_key)\n",
    "\n",
    "    # Create task_telemetry/Will Be phased out\n",
    "    task_telemetry = Listings\n",
    "    \n",
    "    #Create user_telemetry\n",
    "    user_telemetry_data = {\n",
    "        \"TaskName\": current_task[\"TaskName\"],\n",
    "        \"Host\": current_task[\"Host\"],\n",
    "        \"AccountID\": current_user[\"AccountInfo\"][\"AccountID\"]\n",
    "    }\n",
    "\n",
    "    user_telemetry = pd.DataFrame([user_telemetry_data])\n",
    "\n",
    "    return search_results, listing_details, Listings, task_telemetry, user_telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ef023-7982-42e4-b3c2-dffbf00e7f26",
   "metadata": {},
   "source": [
    "## Run Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db68c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Task_Run(driver, current_profile, current_task, current_user, results_check_callback, user_timer):\n",
    "    \"\"\"\n",
    "    Executes a task using the given parameters and handles session management, proxy settings, \n",
    "    and result processing.\n",
    "\n",
    "    Args:\n",
    "        driver (object): The driver used for performing tasks.\n",
    "        current_profile (dict): The current profile details.\n",
    "        current_task (dict): The current task details including search terms.\n",
    "        current_user (dict): Information about the current user including proxy details.\n",
    "        results_check_callback (function): Callback function to check and compare results.\n",
    "        user_timer (object): Timer object to track user account activity.\n",
    "\n",
    "    Yields:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - search_results (DataFrame): DataFrame containing search results.\n",
    "            - listing_details (DataFrame): DataFrame containing details of listings.\n",
    "            - Listings (list): List of processed listings.\n",
    "            - task_telemetry (dict): Dictionary containing task telemetry data.\n",
    "            - user_telemetry (dict): Dictionary containing user telemetry data.\n",
    "            - errors (dict): Dictionary containing error information if any issues are encountered.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Tracks how long the user account has been active using `user_timer.status()`.\n",
    "    2. Creates a session object with updated headers.\n",
    "    3. Configures the session to use a proxy if specified in `current_user`.\n",
    "    4. Checks if the session is blocked using `check_blocked` function.\n",
    "    5. If blocked, formats and yields the final results with errors.\n",
    "    6. Retrieves search listings using `GetSearchListings`.\n",
    "    7. Compares the search results with existing data using `results_check_callback`.\n",
    "    8. Retrieves details for new listings if any.\n",
    "    9. Formats and yields the final results including search results, listing details, \n",
    "       listings, task telemetry, user telemetry, and errors.\n",
    "    \"\"\"\n",
    "    print('Main Funtion: Task_Run')\n",
    "\n",
    "\n",
    "    #How to track how long the user account has been active\n",
    "    user_status = user_timer.status()\n",
    "    \n",
    "    # Create a session object\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "\n",
    "    if current_user['ProxyInfo']['ProxyIp'] != '':\n",
    "        p_ip = current_user['ProxyInfo']['ProxyIp']\n",
    "        p_port = current_user['ProxyInfo']['ProxyPort']\n",
    "        p_user = current_user['ProxyInfo']['ProxyUsername']\n",
    "        p_password = current_user['ProxyInfo']['ProxyPassword']\n",
    "        if p_user is not None and p_user != '':\n",
    "            proxy = {\n",
    "            'http': f\"http://{p_user}:{p_password}@{p_ip}:{p_port}\",\n",
    "            'https': f\"http://{p_user}:{p_password}@{p_ip}:{p_port}\"\n",
    "            }   \n",
    "        else: \n",
    "            proxy = {\n",
    "                'http': f\"http://{p_ip}:{p_port}\",\n",
    "                'https': f\"http://{p_ip}:{p_port}\",\n",
    "            }\n",
    "\n",
    "\n",
    "        # Configure the session to use the proxy\n",
    "        session.proxies.update(proxy)\n",
    "        proxy_set = check_proxy(session, p_ip) \n",
    "        \n",
    "        print('Proxy Set: ', proxy_set)\n",
    "    else:\n",
    "        proxy_set = True\n",
    "\n",
    "\n",
    "    #Check if Blocked\n",
    "    is_blocked, blocked_errors = check_blocked(session)\n",
    "    print(f\"is blocked: {is_blocked}\")\n",
    "    if is_blocked or proxy_set == False:\n",
    "        errors = {\"Disable\": True, \"error_data\": [blocked_errors]}   \n",
    "        print(errors)\n",
    "\n",
    "        final_results, final_details, Listings, task_telemetry, user_telemetry = Format_Output(current_task, current_user, pd.DataFrame(), pd.DataFrame(), [])\n",
    "        yield final_results, final_details, Listings, task_telemetry, user_telemetry, errors\n",
    "        return\n",
    "\n",
    "\n",
    "    search_results_json, errors = GetSearchListings(session, current_task['SearchTerms'], sleep_time=2)\n",
    "    print('JSON', len(search_results_json))\n",
    "    print(errors)\n",
    "\n",
    "    #format data\n",
    "    search_results = pd.DataFrame()\n",
    "    thor_search_results = []\n",
    "    if len(search_results_json):\n",
    "\n",
    "        ##Create Gen 4 DataFrame\n",
    "        search_results = pd.DataFrame(search_results_json)\n",
    "        search_results.reset_index(drop=True, inplace=True)\n",
    "        search_results = search_results.drop_duplicates(subset='id')\n",
    "        print(search_results.shape)\n",
    "        search_results['thor_timestamp'] = int(time.time())\n",
    "        search_results['thor_website'] = \"craigslist.com\"\n",
    "        search_results['thor_search_url'] = current_task['SearchTerms']['Search Text']\n",
    "        search_results['thor_full_listing_url'] = search_results['Link']\n",
    "        search_results['thor_listing_url'] = search_results['Link']\n",
    "\n",
    "        #Create Gen 5 Json\n",
    "        for dictionary in search_results_json:\n",
    "            dictionary['thor_id'] = str(uuid.uuid4())\n",
    "            dictionary['thor_website'] = 'craigslist.com'\n",
    "            dictionary['thor_scraped'] = int(time.time())\n",
    "            dictionary['thor_host'] = current_task['Host']\n",
    "            dictionary['thor_task'] = current_task['TaskName']\n",
    "            dictionary['thor_user'] = current_user['AccountInfo']['AccountID']\n",
    "            dictionary['thor_content'] = json.dumps(dictionary)\n",
    "            thor_search_results.append(dictionary)\n",
    "\n",
    "\n",
    "    # Check For New Listings\n",
    "    print(f\"****************COMPARE DB****************** {len(search_results)} Results\")\n",
    "    compare_columns = {\n",
    "        'id': 'id',\n",
    "    }\n",
    "    new_results = results_check_callback(search_results, 'craigslist.com', compare_columns)\n",
    "    print(f\"****************COMPARE DB Complete****************** {len(new_results)} New\")\n",
    "\n",
    "    #get list of new IDs and ensure no blanks or duplicates\n",
    "    new_ids = new_results['id'].to_list()\n",
    "    new_ids = [*{item for item in new_ids if item}]\n",
    "    new_data = new_results[['SubAreaName', 'AreaName', 'CategoryCode', 'id']].to_dict(orient='records')\n",
    "    print(new_data)\n",
    "\n",
    "    \n",
    "    #Get listing Details\n",
    "    if len(new_results) > 0:\n",
    "        listing_details_json, errors = GetListingDetails(session, new_data, sleep_time=.8)\n",
    "    else:\n",
    "        listing_details_json = []\n",
    "\n",
    "    print(listing_details_json)\n",
    "\n",
    "    thor_listing_details = []\n",
    "    if len(listing_details_json):\n",
    "        #Create Gen 4 DataFrame\n",
    "        listing_details =  pd.json_normalize(listing_details_json)\n",
    "        listing_details['id'] = listing_details['postingId']\n",
    "        listing_details.reset_index(drop=True, inplace=True)\n",
    "        listing_details = listing_details.drop_duplicates(subset='id')\n",
    "        print(listing_details.shape)\n",
    "        \n",
    "        print('Adding standard fields to the DataFrame.')\n",
    "        listing_details['thor_timestamp'] = int(time.time())\n",
    "        listing_details['thor_website'] = \"craigslist.com\"\n",
    "\n",
    "        #Create Gen 5 Json\n",
    "        for dictionary in listing_details_json:\n",
    "            dictionary['thor_id'] = str(uuid.uuid4())\n",
    "            dictionary['thor_website'] = 'craigslist.com'\n",
    "            dictionary['thor_scraped'] = int(time.time())\n",
    "            dictionary['thor_host'] = current_task['Host']\n",
    "            dictionary['thor_task'] = current_task['TaskName']\n",
    "            dictionary['thor_user'] = current_user['AccountInfo']['AccountID']\n",
    "            dictionary['thor_content'] = json.dumps(dictionary)\n",
    "            dictionary['id'] = dictionary['postingId']            \n",
    "            thor_listing_details.append(dictionary)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    #Format standard Output\n",
    "    search_results, listing_details, Listings, task_telemetry, user_telemetry = Format_Output(current_task, current_user,  search_results, listing_details, new_ids)\n",
    "\n",
    "    #filter to only Trucks and Vics we want\n",
    "    # try:\n",
    "    #     filters = thor_filters.listing_filters\n",
    "    #     Listings = thor_filters.apply_filters(Listings, filters)\n",
    "    # except:\n",
    "    #     print('error Filtering')\n",
    "    #     Listings = pd.DataFrame()\n",
    "\n",
    "\n",
    "    print(f\"{len(search_results)} search_results\")\n",
    "    print(f\"{len(listing_details)} listing_details\")\n",
    "    print(f\"{len(Listings)} Listings\")\n",
    "    print(f\"{len(task_telemetry)} task_telemetry\")\n",
    "    print(f\"{len(user_telemetry)} user_telemetry\")\n",
    "    print(f\"{errors} errors\")\n",
    "    errors = {}\n",
    "    \n",
    "    yield search_results, listing_details, Listings, task_telemetry, user_telemetry, errors, thor_search_results, thor_listing_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74d051",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_check(data, website, columns_map):\n",
    "    new_rows = data[:100]#pd.DataFrame(columns=data.columns)\n",
    "    return new_rows\n",
    "\n",
    "class ProcessTimer:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"Process started at {self._format_time(self.start_time)}\")\n",
    "\n",
    "    def finish(self):\n",
    "        self.end_time = time.time()\n",
    "        active_time = self.end_time - self.start_time\n",
    "        result = {\n",
    "            'start_time': self._format_time(self.start_time),\n",
    "            'end_time': self._format_time(self.end_time),\n",
    "            'active_time': str(timedelta(seconds=active_time))\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def status(self):\n",
    "        current_time = time.time()\n",
    "        active_time = current_time - self.start_time\n",
    "        return str(timedelta(seconds=active_time))\n",
    "\n",
    "    def _format_time(self, timestamp):\n",
    "        return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    \n",
    "    \n",
    "# App Settings\n",
    "app_settings = {\n",
    "    'UserProfilesPath': r'\\User Profiles',\n",
    "    'PluginsPath': r'\\Plugins',\n",
    "    'WebRtcName': 'WebRTC',\n",
    "    'DefaultBrowserArgs': [\n",
    "         '--disable-gpu',\n",
    "         '--disable-software-rasterizer'\n",
    "    ]\n",
    "}\n",
    "\n",
    "current_user = {\n",
    "  \"AccountInfo\": {\n",
    "    \"AccountID\": \"craigslist_test_1\",\n",
    "    \"AccountSite\": \"craigslist.com\",\n",
    "    \"AltUserName\": \"\",\n",
    "    \"Description\": \"\",\n",
    "    \"MFAKey\": \"\",\n",
    "    \"OtherData\": {},\n",
    "    \"Password\": \"\",\n",
    "    \"ProfileURL\": \"\",\n",
    "    \"UserName\": \"\"\n",
    "  },\n",
    "  \"ActiveTime\": \"00:00:00\",\n",
    "  \"BrowserInfo\": {\n",
    "    \"Args\": [\n",
    "      \"--disable-notifications\"\n",
    "    ],\n",
    "    \"BrowserType\": \"None\",\n",
    "    \"ClearCookies\": False,\n",
    "    \"Cookies\": [],\n",
    "    \"Description\": \"\",\n",
    "    \"Extensions\": [],\n",
    "    \"Incognito\": False,\n",
    "    \"UserAgent\": \"\",\n",
    "    \"UserFolderName\": \"\",\n",
    "    \"WindowHight\": 0,\n",
    "    \"WindowMaximized\": False,\n",
    "    \"WindowWidth\": 0\n",
    "  },\n",
    "  \"CheckedOut\": False,\n",
    "  \"Disabled\": False,\n",
    "  \"Errors\": [],\n",
    "  \"Host\": \"Bot-Host-10\",\n",
    "  \"LastActive\": \"2024-07-04 17:00:04\",\n",
    "  \"MaxActiveTime\": \"00:01:00\",\n",
    "  \"MinCoolDownTime\": \"00:10:00\",\n",
    "  \"ProxyInfo\": {\n",
    "    \"Description\": \"\",\n",
    "    \"ProxyIp\": \"\",\n",
    "    \"ProxyLoginUrl\": \"\",\n",
    "    \"ProxyPassword\": \"\",\n",
    "    \"ProxyPort\": \"\",\n",
    "    \"ProxyUsername\": \"\"\n",
    "  },\n",
    "  \"Website\": \"craigslist.com\",\n",
    "  \"user_id\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "current_task =  {\n",
    "    'SearchTerms': {\n",
    "        'Search Text': 'tesla model y'\n",
    "    },\n",
    "    'Host': 'Bot-Host-0',\n",
    "    'Website': 'craigslist.com',\n",
    "    'ScriptName': 'Task_Run',\n",
    "    'ProfileName': 'craigslist',\n",
    "    'TaskName': 'All Diesel Vehicles',\n",
    "    'Description': 'All Diesel Vehicles',\n",
    "    'Schedule': {\n",
    "        'DailyEndTime': '23:59:59',\n",
    "        'Interval': '00:50:00',\n",
    "        'DailyStartTime': '00:48:00',\n",
    "        'HitTimes': [],\n",
    "        'IsActive': True\n",
    "        }\n",
    "    }\n",
    "\n",
    "current_profile = {\n",
    "    \"Hosts\": [\"Bot-Host-0\"],\n",
    "    \"ProfileName\": 'craigslist',\n",
    "    \"Website\": 'craigslist.com',\n",
    "    \"Script\": \"craigslist_scripts\",\n",
    "    \"Description\": \"Master Profile For Craigslist\"\n",
    "}  \n",
    "    \n",
    "    \n",
    "ddd = ProcessTimer()\n",
    "for final_results, final_details, Listings, task_telemetry, user_telemetry, errors, search_results_json, thor_listing_details in Task_Run(None, current_profile, current_task, current_user, results_check, ddd):\n",
    "    # print('final_results')\n",
    "    # display(final_results)\n",
    "    # print('final_details')\n",
    "    # display(final_details)\n",
    "    print('Listings')\n",
    "    display(Listings)\n",
    "    # print('task_telemetry')\n",
    "    # display(task_telemetry)\n",
    "    # print('user_telemetry')\n",
    "    # display(user_telemetry)\n",
    "    # print('errors')\n",
    "    search_results_json\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5264d6",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_urls = [\n",
    "#     \"https://wichita.craigslist.org/search/sss?bundleDuplicates=1&postal=67207&auto_fuel_type={Fuel Type}&bundleDuplicates=1&max_auto_miles={Max Miles}&max_auto_year={Max Year}&min_auto_year={Min Year}&purveyor=owner&query={Make}+{Model}&searchNearby=2&sort=date&search_distance=1000&sort=date#search=1~gallery~0~0\",\n",
    "#     \"https://washingtondc.craigslist.org/search/sss?bundleDuplicates=1&postal=20009&auto_fuel_type={Fuel Type}&bundleDuplicates=1&max_auto_miles={Max Miles}&max_auto_year={Max Year}&min_auto_year={Min Year}&purveyor=owner&query={Make}+{Model}&searchNearby=2&sort=date%20F250&search_distance=1000&sort=date#search=1~gallery~0~0\",\n",
    "#     \"https://fresno.craigslist.org/search/sss?bundleDuplicates=1&postal=93728&auto_fuel_type={Fuel Type}&bundleDuplicates=1&max_auto_miles={Max Miles}&max_auto_year={Max Year}&min_auto_year={Min Year}&purveyor=owner&query={Make}+{Model}&searchNearby=2&sort=date%20F250&search_distance=1000&sort=date#search=1~gallery~0~0\"\n",
    "# ]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
